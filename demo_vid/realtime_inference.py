import cv2
import torch
import torchvision.transforms as transforms
from resnet import ResNetFeatureExtractor
from lstm import ActionRecognitionLSTM
import time

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ ë¡œë“œ
cnn = ResNetFeatureExtractor(feature_dim=512).to(DEVICE)
lstm = ActionRecognitionLSTM(input_size=512, hidden_size=256, num_layers=1, num_classes=6).to(DEVICE)

checkpoint = torch.load("action_recognition.pth", map_location=DEVICE)
cnn.load_state_dict(checkpoint['cnn_state_dict'])
lstm.load_state_dict(checkpoint['lstm_state_dict'])

cnn.eval()
lstm.eval()

label_map = {
    0: "throw",
    1: "sit down",
    2: "stand up",
    3: "clapping",
    4: "kick",
    5: "headbanging"
}

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def predict_from_frames(frames):
    max_frames = len(frames)
    video_tensor = torch.stack(frames).unsqueeze(0).to(DEVICE)  # (1, T, 3, 224, 224)

    features = torch.zeros(1, max_frames, 512).to(DEVICE)
    for t in range(max_frames):
        features[:, t, :] = cnn(video_tensor[:, t, :, :, :])

    with torch.no_grad():
        output = lstm(features)
        _, predicted = torch.max(output, 1)

    return predicted.item()

def real_time_action_recognition():
    cap = cv2.VideoCapture(0)

    if not cap.isOpened():
        print("âš ï¸ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¥ì¹˜ë¥¼ ì‹œë„í•˜ê±°ë‚˜ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")
        exit()

    frame_buffer = []
    sequence_length = 30

    print("ğŸ¥ ì‹¤ì‹œê°„ ë™ì‘ ì¸ì‹ ì‹œì‘ (ì¢…ë£Œ: Q í‚¤)")
    while True:
        ret, frame = cap.read()
        if not ret:
            print("âš ï¸ í”„ë ˆì„ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            break

        frame = cv2.flip(frame, 1)  # ì¢Œìš° ë°˜ì „
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        tensor_frame = transform(rgb_frame)

        # í”„ë ˆì„ ë²„í¼ ìœ ì§€
        frame_buffer.append(tensor_frame)
        if len(frame_buffer) > sequence_length:
            frame_buffer.pop(0)  # ì˜¤ë˜ëœ í”„ë ˆì„ ì‚­ì œ

        if len(frame_buffer) == sequence_length:
            predicted_class = predict_from_frames(frame_buffer)
            action_name = label_map[predicted_class]
            print(f"ğŸ“Œ ì˜ˆì¸¡ëœ í–‰ë™: {action_name}")

            # í™”ë©´ì— ì¶œë ¥
            cv2.putText(frame, f"Action: {action_name}", (10, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)

        # í™”ë©´ì— ì¶œë ¥
        cv2.imshow("Real-Time Action Recognition", frame)

        # ì¢…ë£Œ ì¡°ê±´
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

# ì‹¤í–‰
if __name__ == "__main__":
    real_time_action_recognition()
