import cv2
import torch
import numpy as np
import torchvision.transforms as transforms
from resnet import ResNetFeatureExtractor
from lstm import ActionRecognitionLSTM
import time

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ ë¡œë“œ
cnn = ResNetFeatureExtractor(feature_dim=512).to(DEVICE)
lstm = ActionRecognitionLSTM(input_size=512, hidden_size=256, num_layers=1, num_classes=6).to(DEVICE)

# ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
checkpoint = torch.load("action_recognition.pth")
cnn.load_state_dict(checkpoint['cnn_state_dict'])
lstm.load_state_dict(checkpoint['lstm_state_dict'])
cnn.eval()
lstm.eval()

# ë¼ë²¨ ì¸ë±ìŠ¤ â†’ í–‰ë™ ì´ë¦„ ë§¤í•‘
label_map = {
    0: "throw",
    1: "sit down",
    2: "stand up",
    3: "clapping",
    4: "kick",
    5: "headbanging"
}

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def realtime_action_recognition():
    # ì›¹ìº  ì—´ê¸°
    cap = cv2.VideoCapture(0)  # 0ì€ ê¸°ë³¸ ì›¹ìº 
    
    if not cap.isOpened():
        print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    frame_buffer = []
    max_frames = 50
    prediction = None
    last_prediction_time = time.time()
    prediction_interval = 1.0  # 1ì´ˆë§ˆë‹¤ ì˜ˆì¸¡
    
    # OpenCV ì°½ ìƒì„±
    cv2.namedWindow("Real-time Action Recognition", cv2.WINDOW_NORMAL)
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì›¹ìº ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        # í™”ë©´ì— ë³´ì—¬ì£¼ê¸° ìœ„í•œ ì›ë³¸ í”„ë ˆì„ ë³µì‚¬
        display_frame = frame.copy()
        
        # í”„ë ˆì„ ì „ì²˜ë¦¬
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        processed_frame = transform(rgb_frame)
        
        # í”„ë ˆì„ ë²„í¼ ê´€ë¦¬
        frame_buffer.append(processed_frame)
        if len(frame_buffer) > max_frames:
            frame_buffer.pop(0)
        
        current_time = time.time()
        
        # ì¶©ë¶„í•œ í”„ë ˆì„ì´ ìŒ“ì´ê³  ì¼ì • ì‹œê°„ì´ ê²½ê³¼í•˜ë©´ ì˜ˆì¸¡ ìˆ˜í–‰
        if len(frame_buffer) >= max_frames and (current_time - last_prediction_time) >= prediction_interval:
            # ë¶€ì¡±í•œ í”„ë ˆì„ì€ ë§ˆì§€ë§‰ í”„ë ˆì„ìœ¼ë¡œ ì±„ìš°ê¸° (ì´ ê²½ìš°ëŠ” í•„ìš” ì—†ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´)
            if len(frame_buffer) < max_frames:
                last = frame_buffer[-1]
                frame_buffer.extend([last] * (max_frames - len(frame_buffer)))
            
            # (1, max_frames, 3, 224, 224) í˜•íƒœë¡œ ë°°ì¹˜ êµ¬ì„±
            video_tensor = torch.stack(frame_buffer[-max_frames:]).unsqueeze(0).to(DEVICE)
            
            # CNN -> Feature ì¶”ì¶œ
            features = torch.zeros(1, max_frames, 512).to(DEVICE)
            for t in range(max_frames):
                with torch.no_grad():
                    features[:, t, :] = cnn(video_tensor[:, t, :, :, :])
            
            # LSTM -> ì˜ˆì¸¡
            with torch.no_grad():
                output = lstm(features)
                _, predicted = torch.max(output, 1)
            
            prediction = predicted.item()
            print(f"ğŸ” ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì¸ë±ìŠ¤: {prediction}")
            print(f"ğŸ“Œ ì˜ˆì¸¡ëœ í–‰ë™: {label_map[prediction]}")
            
            last_prediction_time = current_time
        
        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™”ë©´ì— í‘œì‹œ
        if prediction is not None:
            action_text = f"Action: {label_map[prediction]}"
            cv2.putText(display_frame, action_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # FPS ê³„ì‚° ë° í‘œì‹œ
        fps_text = f"FPS: {int(1 / (time.time() - current_time + 0.001))}"
        cv2.putText(display_frame, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # ì…ë ¥ ë²„í¼ í”„ë ˆì„ ìˆ˜ í‘œì‹œ
        buffer_text = f"Buffer: {len(frame_buffer)}/{max_frames}"
        cv2.putText(display_frame, buffer_text, (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # í”„ë ˆì„ í™”ë©´ì— í‘œì‹œ
        cv2.imshow("Real-time Action Recognition", display_frame)
        
        # 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    # ìì› í•´ì œ
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    realtime_action_recognition()